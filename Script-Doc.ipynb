{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b72e52e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload \n",
    "%autoreload 2\n",
    "\n",
    "!pip install --upgrade boto3\n",
    "!pip install --upgrade sagemaker\n",
    "!pip install python_docx\n",
    "!pip install langchain\n",
    "!pip install pypdf\n",
    "!pip install docx2txt\n",
    "!pip install unstructured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca88125",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.document_loaders import UnstructuredMarkdownLoader\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.document_loaders import Docx2txtLoader\n",
    "from langchain.document_loaders import UnstructuredPowerPointLoader\n",
    "from langchain.document_loaders.csv_loader import CSVLoader\n",
    "from langchain.document_loaders import UnstructuredExcelLoader\n",
    "from langchain.text_splitter import NLTKTextSplitter\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from chinese_text_splitter import ChineseTextSplitter\n",
    "import json\n",
    "from typing import Dict, List, Tuple\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "import boto3\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "import requests\n",
    "\n",
    "nltk.download('punkt')\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.CRITICAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c462bf8",
   "metadata": {},
   "source": [
    "## Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7e860b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The name of index\n",
    "sm_client = boto3.client('secretsmanager')\n",
    "index_name = sm_client.get_secret_value(SecretId='opensearch-index-name')['SecretString']\n",
    "data= json.loads(index_name)\n",
    "index_name = data.get('index')\n",
    "print('pre-defined index name in deployment/cdk.json-->',index_name)\n",
    "#index_name = ''\n",
    "\n",
    "# Language, 'chinese' or 'english'\n",
    "language = 'chinese'\n",
    "\n",
    "# The name of embbeding model endpoint, usually you can keep it as default\n",
    "eb_endpoint = 'huggingface-inference-eb'\n",
    "\n",
    "# Ebbeding vector dimension, usually you can keep it as default\n",
    "v_dimension = 768\n",
    "\n",
    "# Docs file folder to be processed and ingested\n",
    "folder_path = 'docs/'\n",
    "print('Please put data in this folder-->',folder_path)\n",
    "\n",
    "# Paragraph size / Chunck size\n",
    "chunck_size = 200\n",
    "\n",
    "# The imported data of the same index_name, usually you can keep it as 0 if you are creating a new index\n",
    "before_import = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c82381",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hfp = sagemaker.huggingface.model.HuggingFacePredictor(eb_endpoint)\n",
    "\n",
    "#===================Function Definition=================\n",
    "\n",
    "def load_file(filepath,language):\n",
    "    \n",
    "    if filepath.lower().endswith(\".pdf\"):\n",
    "        loader = PyPDFLoader(filepath)\n",
    "    elif filepath.lower().endswith(\".docx\"):\n",
    "        loader = Docx2txtLoader(filepath)\n",
    "    elif filepath.lower().endswith(\".pptx\"):\n",
    "        loader = UnstructuredPowerPointLoader(filepath)\n",
    "    elif filepath.lower().endswith(\".csv\"):\n",
    "        loader = CSVLoader(filepath)\n",
    "    elif filepath.lower().endswith(\".xlsx\"):\n",
    "        loader = UnstructuredExcelLoader(filepath)\n",
    "    elif filepath.lower().endswith(\".txt\"):\n",
    "        loader = TextLoader(filepath)\n",
    "    else:\n",
    "        loader = TextLoader(filepath)\n",
    "\n",
    "    if language == \"chinese\":\n",
    "        textsplitter = ChineseTextSplitter()\n",
    "    elif language == \"english\":\n",
    "        textsplitter = NLTKTextSplitter(chunk_size=chunck_size, chunk_overlap=10)\n",
    "\n",
    "    docs = loader.load_and_split(textsplitter)\n",
    "    return docs\n",
    "\n",
    "\n",
    "def get_title(path):\n",
    "    try:\n",
    "        title = os.path.split(os.path.splitext(path)[0])[1].replace('——', '-').split('-')[1]\n",
    "    except:\n",
    "        title = os.path.split(os.path.splitext(path)[0])[1]\n",
    "    return title\n",
    "\n",
    "def read_doc(path, chunck_size = chunck_size):\n",
    "    doc = load_file(path, language)\n",
    "    title = get_title(path)\n",
    "    titles = []\n",
    "    paragraphs = []\n",
    "    sentences = []\n",
    "    para = ''\n",
    "    con = 0\n",
    "    for d in doc:\n",
    "#         print('*********')\n",
    "        con += 1\n",
    "        titles.append(title)\n",
    "        sentences.append(d.page_content)\n",
    "        para += d.page_content\n",
    "        if len(para) >= chunck_size:\n",
    "            paragraphs += [para for _ in range(con)]\n",
    "            para = ''\n",
    "            con = 0\n",
    "    paragraphs += [para for _ in range(con)]\n",
    "    print(len(titles), len(sentences),len(paragraphs))\n",
    "    df = pd.DataFrame({'title':titles, 'paragraph':paragraphs, 'sentence':sentences})\n",
    "    return df\n",
    "\n",
    "def get_vector(q):\n",
    "    try:\n",
    "        vector = hfp.predict({'inputs':[q]})[0][0][0]\n",
    "        return vector\n",
    "    except:\n",
    "        return [-1000 for _ in range(v_dimension)]\n",
    "    return hfp.predict({'inputs':[q]})[0][0][0]\n",
    "\n",
    "def embbeding(df):\n",
    "    df['title_vector'] = ''\n",
    "    df['sentence_vector'] = ''\n",
    "    title_vector = str(get_vector(df.iloc[0, 0]))\n",
    "    for i in range(len(df)):\n",
    "#         df.iloc[i, 5] = title_vector\n",
    "        df.iloc[i, 3] = str(get_vector(df.iloc[i, 2]))\n",
    "        print('\\r embbeding %i out of %i finished'%(i, len(df)), end='')\n",
    "    return df\n",
    "\n",
    "# ==============OpenSearch Related=====================\n",
    "# retrieve secret manager value by key using boto3\n",
    "sm_client = boto3.client('secretsmanager')\n",
    "master_user = sm_client.get_secret_value(SecretId='opensearch-host-url')['SecretString']\n",
    "data= json.loads(master_user)\n",
    "es_host_name = data.get('host')\n",
    "host = es_host_name+'/' if es_host_name[-1] != '/' else es_host_name# cluster endpoint, for example: my-test-domain.us-east-1.es.amazonaws.com/\n",
    "region = boto3.Session().region_name # e.g. cn-north-1\n",
    "# sm_client = boto3.client('secretsmanager')\n",
    "master_user = sm_client.get_secret_value(SecretId='opensearch-master-user')['SecretString']\n",
    "data= json.loads(master_user)\n",
    "username = data.get('username')\n",
    "password = data.get('password')\n",
    "# service = 'es'\n",
    "# credentials = boto3.Session().get_credentials()\n",
    "awsauth = (username, password)\n",
    "url = host+'_bulk'\n",
    "headers = { \"Content-Type\": \"application/json\" }\n",
    "\n",
    "payloads = {\n",
    "\"settings\": { \"index\": {\n",
    "\"knn\": True,\n",
    "\"knn.algo_param.ef_search\": 100 }\n",
    "}, \"mappings\": {\n",
    "\"properties\": { \n",
    "  \"title_vector\": {\n",
    "\"type\": \"knn_vector\", \"dimension\": v_dimension, \"method\": {\n",
    "\"name\": \"hnsw\", \"space_type\": \"l2\", \"engine\": \"nmslib\", \"parameters\": {\n",
    "\"ef_construction\": 256,\n",
    "\"m\": 128 }\n",
    "} },\n",
    "\"sentence_vector\": {\n",
    "\"type\": \"knn_vector\", \"dimension\": v_dimension, \"method\": {\n",
    "\"name\": \"hnsw\", \"space_type\": \"l2\", \"engine\": \"nmslib\", \"parameters\": {\n",
    "\"ef_construction\": 256,\n",
    "\"m\": 128 }\n",
    "} },\n",
    "\"title\": { \"type\": \"text\"}, \n",
    "\"sentence\": {\"type\": \"text\" }, \n",
    "\"paragraph\": {\"type\": \"text\" }, \n",
    "\"sentence_id\": {\"type\": \"text\" }, \n",
    "\"paragraph_id\": {\"type\": \"text\" }\n",
    "} }\n",
    "}\n",
    "\n",
    "# Create Index\n",
    "r = requests.put(host+index_name, auth=awsauth, headers=headers, json=payloads)\n",
    "\n",
    "def import_data(df, id_start=0, before_import=0):\n",
    "    payloads = ''\n",
    "    for i in range(id_start, len(df)+id_start):\n",
    "        first = json.dumps({ \"index\": { \"_index\": index_name, \"_id\": str(i+before_import) } }, ensure_ascii=False) + \"\\n\"\n",
    "        second = json.dumps({\"title\": str(df.iloc[i-id_start, 0]), \n",
    "                     \"paragraph\": str(df.iloc[i-id_start, 1]), \n",
    "                     \"sentence\": str(df.iloc[i-id_start, 2]), \n",
    "                     \"sentence_vector\": json.loads(df.iloc[i-id_start, 3])},\n",
    "                   ensure_ascii=False) + \"\\n\"\n",
    "        payloads += first + second\n",
    "    # print(payloads)\n",
    "    r = requests.post(url, auth=awsauth, headers=headers, data=payloads.encode()) # requests.get, post, and delete have similar syntax\n",
    "#     print(r.text)\n",
    "\n",
    "#==============Main Preprocess Data and Import===============\n",
    "\n",
    "slice = 10\n",
    "names = os.listdir(folder_path)\n",
    "# before_import = 0\n",
    "failed_files = []\n",
    "for j in range(len(names)):\n",
    "    name = names[j]\n",
    "#     if os.path.splitext(name)[1] not in ['.doc','.docx']:continue\n",
    "    print('filename-->',name)\n",
    "    if os.path.isdir(name):continue\n",
    "    try:\n",
    "        df = read_doc(os.path.join(folder_path, name))\n",
    "        df = embbeding(df)\n",
    "        for i in range(len(df)//slice+1):\n",
    "            import_data(df[slice*i:slice*(i+1)], slice*i, before_import)\n",
    "            print('\\r import %i out of %i finished'%(i, len(df)//slice+1), end='')\n",
    "        before_import += len(df)\n",
    "        print(' file %i out of %i finished'%(j, len(names)//slice+1))\n",
    "    except Exception as ex:\n",
    "        # traceback.print_exc(file=sys.stdout)\n",
    "        failed_files.append(name)\n",
    "        print(f\"=================Exception================={ex}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a8e622",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839ee97e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
