{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33d70eb6-070e-4d84-9c8f-5ec3572a74fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import boto3\n",
    "\n",
    "# client = boto3.client('sagemaker')\n",
    "\n",
    "# response = client.delete_endpoint_config(EndpointConfigName='pytorch-inference-baichuan-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6c8076-0b56-4ec8-9884-37f1ec78b106",
   "metadata": {
    "tags": []
   },
   "source": [
    "# huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfba5a10-46a1-4502-8bd1-332e6ac69a84",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:iam::310850127430:role/anbei\n",
      "sagemaker-us-west-2-310850127430\n",
      "us-west-2\n",
      "a dummy\n",
      "upload: ./model.tar.gz to s3://sagemaker-us-west-2-310850127430/llm_chinese/assets/model.tar.gz\n"
     ]
    },
    {
     "ename": "ClientError",
     "evalue": "An error occurred (ValidationException) when calling the CreateModel operation: Could not access model data at s3://sagemaker-us-west-2-310850127430/huggingface-pytorch-inference-2023-08-08-08-24-35-710/model.tar.gz. Please ensure that the role \"arn:aws:iam::310850127430:role/anbei\" exists and that its trust relationship policy allows the action \"sts:AssumeRole\" for the service principal \"sagemaker.amazonaws.com\". Also ensure that the role has \"s3:GetObject\" permissions and that the object is located in us-west-2. If your Model uses multiple models or uncompressed models, please ensure that the role has \"s3:ListBucket\" permission.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mClientError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 69\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msagemaker\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mserializers\u001b[39;00m \u001b[39mimport\u001b[39;00m JSONSerializer\n\u001b[1;32m     68\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msagemaker\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdeserializers\u001b[39;00m \u001b[39mimport\u001b[39;00m JSONDeserializer\n\u001b[0;32m---> 69\u001b[0m predictor \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mdeploy(\n\u001b[1;32m     70\u001b[0m     endpoint_name \u001b[39m=\u001b[39;49m endpoint_name,\n\u001b[1;32m     71\u001b[0m     instance_type \u001b[39m=\u001b[39;49m instance_type, \n\u001b[1;32m     72\u001b[0m     initial_instance_count \u001b[39m=\u001b[39;49m instance_count,\n\u001b[1;32m     73\u001b[0m     serializer \u001b[39m=\u001b[39;49m JSONSerializer(),\n\u001b[1;32m     74\u001b[0m     deserializer \u001b[39m=\u001b[39;49m JSONDeserializer()\n\u001b[1;32m     75\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/sagemaker/huggingface/model.py:311\u001b[0m, in \u001b[0;36mHuggingFaceModel.deploy\u001b[0;34m(self, initial_instance_count, instance_type, serializer, deserializer, accelerator_type, endpoint_name, tags, kms_key, wait, data_capture_config, async_inference_config, serverless_inference_config, volume_size, model_data_download_timeout, container_startup_health_check_timeout, inference_recommendation_id, explainer_config, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimage_uri \u001b[39mand\u001b[39;00m instance_type \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m instance_type\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39mml.inf\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    306\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimage_uri \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mserving_image_uri(\n\u001b[1;32m    307\u001b[0m         region_name\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msagemaker_session\u001b[39m.\u001b[39mboto_session\u001b[39m.\u001b[39mregion_name,\n\u001b[1;32m    308\u001b[0m         instance_type\u001b[39m=\u001b[39minstance_type,\n\u001b[1;32m    309\u001b[0m     )\n\u001b[0;32m--> 311\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m(HuggingFaceModel, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mdeploy(\n\u001b[1;32m    312\u001b[0m     initial_instance_count,\n\u001b[1;32m    313\u001b[0m     instance_type,\n\u001b[1;32m    314\u001b[0m     serializer,\n\u001b[1;32m    315\u001b[0m     deserializer,\n\u001b[1;32m    316\u001b[0m     accelerator_type,\n\u001b[1;32m    317\u001b[0m     endpoint_name,\n\u001b[1;32m    318\u001b[0m     tags,\n\u001b[1;32m    319\u001b[0m     kms_key,\n\u001b[1;32m    320\u001b[0m     wait,\n\u001b[1;32m    321\u001b[0m     data_capture_config,\n\u001b[1;32m    322\u001b[0m     async_inference_config,\n\u001b[1;32m    323\u001b[0m     serverless_inference_config,\n\u001b[1;32m    324\u001b[0m     volume_size\u001b[39m=\u001b[39;49mvolume_size,\n\u001b[1;32m    325\u001b[0m     model_data_download_timeout\u001b[39m=\u001b[39;49mmodel_data_download_timeout,\n\u001b[1;32m    326\u001b[0m     container_startup_health_check_timeout\u001b[39m=\u001b[39;49mcontainer_startup_health_check_timeout,\n\u001b[1;32m    327\u001b[0m     inference_recommendation_id\u001b[39m=\u001b[39;49minference_recommendation_id,\n\u001b[1;32m    328\u001b[0m     explainer_config\u001b[39m=\u001b[39;49mexplainer_config,\n\u001b[1;32m    329\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/sagemaker/model.py:1260\u001b[0m, in \u001b[0;36mModel.deploy\u001b[0;34m(self, initial_instance_count, instance_type, serializer, deserializer, accelerator_type, endpoint_name, tags, kms_key, wait, data_capture_config, async_inference_config, serverless_inference_config, volume_size, model_data_download_timeout, container_startup_health_check_timeout, inference_recommendation_id, explainer_config, **kwargs)\u001b[0m\n\u001b[1;32m   1257\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_base_name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1258\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_base_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m-\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin((\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_base_name, compiled_model_suffix))\n\u001b[0;32m-> 1260\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_sagemaker_model(\n\u001b[1;32m   1261\u001b[0m     instance_type, accelerator_type, tags, serverless_inference_config\n\u001b[1;32m   1262\u001b[0m )\n\u001b[1;32m   1264\u001b[0m serverless_inference_config_dict \u001b[39m=\u001b[39m (\n\u001b[1;32m   1265\u001b[0m     serverless_inference_config\u001b[39m.\u001b[39m_to_request_dict() \u001b[39mif\u001b[39;00m is_serverless \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1266\u001b[0m )\n\u001b[1;32m   1267\u001b[0m production_variant \u001b[39m=\u001b[39m sagemaker\u001b[39m.\u001b[39mproduction_variant(\n\u001b[1;32m   1268\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname,\n\u001b[1;32m   1269\u001b[0m     instance_type,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1275\u001b[0m     container_startup_health_check_timeout\u001b[39m=\u001b[39mcontainer_startup_health_check_timeout,\n\u001b[1;32m   1276\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/sagemaker/model.py:731\u001b[0m, in \u001b[0;36mModel._create_sagemaker_model\u001b[0;34m(self, instance_type, accelerator_type, tags, serverless_inference_config)\u001b[0m\n\u001b[1;32m    718\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_enable_network_isolation \u001b[39m=\u001b[39m resolve_value_from_config(\n\u001b[1;32m    719\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_enable_network_isolation,\n\u001b[1;32m    720\u001b[0m     MODEL_ENABLE_NETWORK_ISOLATION_PATH,\n\u001b[1;32m    721\u001b[0m     sagemaker_session\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msagemaker_session,\n\u001b[1;32m    722\u001b[0m )\n\u001b[1;32m    723\u001b[0m create_model_args \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(\n\u001b[1;32m    724\u001b[0m     name\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname,\n\u001b[1;32m    725\u001b[0m     role\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrole,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    729\u001b[0m     tags\u001b[39m=\u001b[39mtags,\n\u001b[1;32m    730\u001b[0m )\n\u001b[0;32m--> 731\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msagemaker_session\u001b[39m.\u001b[39;49mcreate_model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcreate_model_args)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/sagemaker/session.py:3404\u001b[0m, in \u001b[0;36mSession.create_model\u001b[0;34m(self, name, role, container_defs, vpc_config, enable_network_isolation, primary_container, tags)\u001b[0m\n\u001b[1;32m   3401\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   3402\u001b[0m             \u001b[39mraise\u001b[39;00m\n\u001b[0;32m-> 3404\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_intercept_create_request(create_model_request, submit, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcreate_model\u001b[39m.\u001b[39;49m\u001b[39m__name__\u001b[39;49m)\n\u001b[1;32m   3405\u001b[0m \u001b[39mreturn\u001b[39;00m name\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/sagemaker/session.py:5239\u001b[0m, in \u001b[0;36mSession._intercept_create_request\u001b[0;34m(self, request, create, func_name)\u001b[0m\n\u001b[1;32m   5222\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_intercept_create_request\u001b[39m(\n\u001b[1;32m   5223\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   5224\u001b[0m     request: typing\u001b[39m.\u001b[39mDict,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5227\u001b[0m     \u001b[39m# pylint: disable=unused-argument\u001b[39;00m\n\u001b[1;32m   5228\u001b[0m ):\n\u001b[1;32m   5229\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"This function intercepts the create job request.\u001b[39;00m\n\u001b[1;32m   5230\u001b[0m \n\u001b[1;32m   5231\u001b[0m \u001b[39m    PipelineSession inherits this Session class and will override\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5237\u001b[0m \u001b[39m        func_name (str): the name of the function needed intercepting\u001b[39;00m\n\u001b[1;32m   5238\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 5239\u001b[0m     \u001b[39mreturn\u001b[39;00m create(request)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/sagemaker/session.py:3392\u001b[0m, in \u001b[0;36mSession.create_model.<locals>.submit\u001b[0;34m(request)\u001b[0m\n\u001b[1;32m   3390\u001b[0m LOGGER\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mCreateModel request: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, json\u001b[39m.\u001b[39mdumps(request, indent\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m))\n\u001b[1;32m   3391\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3392\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msagemaker_client\u001b[39m.\u001b[39;49mcreate_model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mrequest)\n\u001b[1;32m   3393\u001b[0m \u001b[39mexcept\u001b[39;00m ClientError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   3394\u001b[0m     error_code \u001b[39m=\u001b[39m e\u001b[39m.\u001b[39mresponse[\u001b[39m\"\u001b[39m\u001b[39mError\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mCode\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/botocore/client.py:530\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    526\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    527\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mpy_operation_name\u001b[39m}\u001b[39;00m\u001b[39m() only accepts keyword arguments.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    528\u001b[0m     )\n\u001b[1;32m    529\u001b[0m \u001b[39m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[0;32m--> 530\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_api_call(operation_name, kwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/botocore/client.py:964\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    962\u001b[0m     error_code \u001b[39m=\u001b[39m parsed_response\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mError\u001b[39m\u001b[39m\"\u001b[39m, {})\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mCode\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    963\u001b[0m     error_class \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexceptions\u001b[39m.\u001b[39mfrom_code(error_code)\n\u001b[0;32m--> 964\u001b[0m     \u001b[39mraise\u001b[39;00m error_class(parsed_response, operation_name)\n\u001b[1;32m    965\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    966\u001b[0m     \u001b[39mreturn\u001b[39;00m parsed_response\n",
      "\u001b[0;31mClientError\u001b[0m: An error occurred (ValidationException) when calling the CreateModel operation: Could not access model data at s3://sagemaker-us-west-2-310850127430/huggingface-pytorch-inference-2023-08-08-08-24-35-710/model.tar.gz. Please ensure that the role \"arn:aws:iam::310850127430:role/anbei\" exists and that its trust relationship policy allows the action \"sts:AssumeRole\" for the service principal \"sagemaker.amazonaws.com\". Also ensure that the role has \"s3:GetObject\" permissions and that the object is located in us-west-2. If your Model uses multiple models or uncompressed models, please ensure that the role has \"s3:ListBucket\" permission."
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "region_name = boto3.session.Session().region_name\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "print(role)\n",
    "print(bucket)\n",
    "print(region_name)\n",
    "\n",
    "\n",
    "if \"cn-\" in region_name:\n",
    "    with open('./code/requirements.txt', 'r') as original: data = original.read()\n",
    "    with open('./code/requirements.txt', 'w') as modified: modified.write(\"-i https://pypi.tuna.tsinghua.edu.cn/simple\\n\" + data)\n",
    "\n",
    "!touch dummy\n",
    "!tar czvf model.tar.gz dummy\n",
    "assets_dir = 's3://{0}/{1}/assets/'.format(bucket, 'llm_chinese')\n",
    "model_data = 's3://{0}/{1}/assets/model.tar.gz'.format(bucket, 'llm_chinese')\n",
    "!aws s3 cp model.tar.gz $assets_dir\n",
    "!rm -f dummy model.tar.gz\n",
    "\n",
    "model_name = None\n",
    "entry_point = 'inference.py'\n",
    "# framework_version = '1.13.1'\n",
    "# py_version = 'py39'\n",
    "model_environment = {\n",
    "    'SAGEMAKER_MODEL_SERVER_TIMEOUT':'420', \n",
    "    'SAGEMAKER_MODEL_SERVER_WORKERS': '1', \n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "url = f'763104351884.dkr.ecr.{region_name}.amazonaws.com/huggingface-pytorch-inference:2.0.0-transformers4.28.1-gpu-py310-cu118-ubuntu20.04'\n",
    "from sagemaker.huggingface.model import HuggingFaceModel\n",
    "model = HuggingFaceModel(\n",
    "    name = model_name,\n",
    "    model_data = model_data,\n",
    "    entry_point = entry_point,\n",
    "    source_dir = './code',\n",
    "    role = role,\n",
    "    # framework_version = framework_version, \n",
    "    # py_version = py_version,\n",
    "    # env = model_environment\n",
    "    image_uri=url\n",
    ")\n",
    "\n",
    "endpoint_name = 'hf-inference-baichuan-v1'\n",
    "instance_type='ml.g5.4xlarge' \n",
    "\n",
    "instance_count = 1\n",
    "\n",
    "\n",
    "import boto3\n",
    "\n",
    "client = boto3.client('sagemaker')\n",
    "try:\n",
    "    response = client.delete_endpoint_config(EndpointConfigName=endpoint_name)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "predictor = model.deploy(\n",
    "    endpoint_name = endpoint_name,\n",
    "    instance_type = instance_type, \n",
    "    initial_instance_count = instance_count,\n",
    "    serializer = JSONSerializer(),\n",
    "    deserializer = JSONDeserializer()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fec371-c19e-46bb-aadf-081a26cae931",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "晚上睡不着可能是因为身体疲劳、压力过大、焦虑、饮食不当等原因。以下是一些帮助您入睡的建议：\n",
      "\n",
      "1. 放松身体：尝试进行深呼吸、冥想或瑜伽等放松活动，以帮助您放松身体和思维。\n",
      "\n",
      "2. 创造一个舒适的睡眠环境：确保您的卧室安静、黑暗、凉爽和舒适。\n",
      "\n",
      "3. 避免刺激性物质：避免饮用咖啡因、酒精和尼古丁等刺激性物质，因为它们会影响您的睡眠。\n",
      "\n",
      "4. 建立规律的睡眠时间：尽量在同一时间入睡和起床，以帮助您的身体建立规律的睡眠模式。\n",
      "\n",
      "5. 避免午睡：如果您需要午睡，请确保在下午3点之前完成，以避免影响晚上的睡眠。\n",
      "\n",
      "6. 避免使用电子设备：在睡觉前避免使用电子设备，因为它们会刺激您的思维和眼睛。\n",
      "\n",
      "7. 寻求帮助：如果您尝试了以上建议但仍然无法入睡，请考虑咨询医生或专业人士的建议。\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "predictor = sagemaker.predictor.Predictor(endpoint_name)\n",
    "predictor.serializer = JSONSerializer()\n",
    "predictor.deserializer = JSONDeserializer()\n",
    "\n",
    "inputs= {\n",
    "    \"ask\": \"晚上睡不着应该怎么办\"\n",
    "\n",
    "}\n",
    "\n",
    "response = predictor.predict(inputs)\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac57b11-7d49-4206-a27f-9e60bbfa5bbb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
